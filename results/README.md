# Prompt Selection
## Prompts
### Task (Dataset) Prompts 
Task prompts were created for each dataset: 
<details>
<summary>dailydialog</summary>

1. "respond to the final sentence: "
2. "continue this dialog: "
3. "finish this dialog: "
4. "continue writing the next sentence in this: "
</details>

<details>
<summary>dailymail_cnn</summary>

1. "summarize the main points of this article: "
2. "create a summary of the news article: "
3. "write a short summarized text of the news article: "
4. "summarize this: "
5. "what are the important parts of this article?: "
6. "write highlights for this article: "
</details>

<details>
<summary>mrpc</summary>

1. "paraphrase this text: "
2. "generate a sentence with a similar meaning: "
3. paraphrase this: "
4. "make a sentence that means the same as this: "
</details>

<details>
<summary>stories</summary>

1. "continue the story: "
2. "write a small text based on this story: "
3. "complete the text: "
4. "complete the story: "
5. "finish the story: "
6. "Make a story based on this writing prompt: "
</details>

### System (Model) Prompts
Each model also had their seperate system prompt that was presented prior to the task prompts:
<details>
<summary>StableBeluga</summary>

You are StableBeluga, an AI that follows instructions extremely well. Help as much as you can. Remember, be safe, and don't do anything illegal.

</details>

<details>
<summary>Llama2 chat </summary>

You are an AI, but you do not deviate from the task prompt, and you do not engage in small talk. Never begin your response with 'Sure, here is my response:' or anything of the like. It is important that you finish without getting cut off.

</details>
<br>
That is, each prompt consisted of a system prompt, followed by a task prompt. The system prompt was kept constant.


## Workflow
For each prompt for the particular dataset, 100 examples were generated by both Llama2 chat (13b) and StableBeluga (7B). 

Low-level features `["doc_length", "n_tokens", "n_characters", "n_sentences"]` were extracted for each prompt for each model.  

Afterwards, PCA was run on these low-level features to get new PC components. See the `PCA` folder for those results.

To then investigate differences between model completions and human completions, euclidean distances were computed between the PC components of the human generation and each model generation (i.e., human-beluga, human-llama2_chat). See also [src/prompt_selection/distance.py](https://github.com/rbroc/echo/blob/main/src/prompt_selection/distance.py).

## Plotting
Interactive plots illustrate the distance scores and their corresponding completion by hovering over them:
1. [dailydialog](https://htmlpreview.github.io/?https://github.com/rbroc/echo/blob/main/results/distance/all_PC_jitterplots/interactive/dailydialog.html)
2. [dailymail_cnn](https://htmlpreview.github.io/?https://github.com/rbroc/echo/blob/main/results/distance/all_PC_jitterplots/interactive/dailymail_cnn.html)
3. [mrpc](https://htmlpreview.github.io/?https://github.com/rbroc/echo/blob/main/results/distance/all_PC_jitterplots/interactive/mrpc.html)
4. [stories](https://htmlpreview.github.io/?https://github.com/rbroc/echo/blob/main/results/distance/all_PC_jitterplots/interactive/stories.html)

Static plots can also be found in this [folder](https://github.com/rbroc/echo/tree/main/results/distance/all_PC_jitterplots/static).

## Medians
The medians of the distances were computed for each model, dataset and prompt number:

| dataset       | model       |   1.0 |   2.0 |   3.0 |   4.0 | 5.0   | 6.0   |
|---------------|-------------|-------|-------|-------|-------|-------|-------|
|               | beluga      |  0.39 |  0.52 |  0.40 |  0.27 |       |       |
| dailydialog   | llama2_chat |  1.16 |  1.38 |  1.42 |  1.07 |       |       |
|               | beluga      |  0.61 |  0.46 |  0.49 |  0.62 | 1.11  | 0.664 |
| dailymail_cnn | llama2_chat |  1.67 |  1.48 |  1.31 |  1.39 | 1.803 | 2.001 |
|               | beluga      |  0.04 |  0.05 |  0.05 |  0.05 |       |       |
| mrpc          | llama2_chat |  0.07 |  0.07 |  0.06 |  0.06 |       |       |
|               | beluga      |  2.83 |  2.98 |  5.05 |  3.14 | 3.533 | 2.875 |
| stories       | llama2_chat |  3.07 |  3.41 |  3.83 |  3.03 | 2.871 | 2.535 |

## Two lowest medians per MODEL, DATASET
We can also group the results in the two prompts that hold the lowest median per model and dataset: 
| dataset       | model       |   prompt |   median |
|---------------|-------------|----------|----------|
| dailydialog   | beluga      |        4 |    0.267 |
|               |             |        1 |    0.394 |
|               | llama2_chat |        4 |    1.074 |
|               |             |        1 |    1.162 |
| dailymail_cnn | beluga      |        2 |    0.46  |
|               |             |        3 |    0.487 |
|               | llama2_chat |        3 |    1.311 |
|               |             |        4 |    1.394 |
| mrpc          | beluga      |        1 |    0.045 |
|               |             |        3 |    0.046 |
|               | llama2_chat |        4 |    0.064 |
|               |             |        3 |    0.064 |
| stories       | beluga      |        1 |    2.832 |
|               |             |        6 |    2.875 |
|               | llama2_chat |        6 |    2.535 |
|               |             |        5 |    2.871 |